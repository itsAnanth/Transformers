{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "450dafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "A\n",
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:150])\n",
    "print(f\"Length of text: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8e7fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "n_vocab = len(chars)\n",
    "\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(f\"Vocabulary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb8eb5",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "We have a custom tokenizer, its a character level tokenizer for the sake of simplicity\n",
    "\n",
    "Some popular tokenizers includes tiktoken (byte pair encoding), sentencepiece (sub word unit encoding)\n",
    "\n",
    "The above stated tokenizers have very large vocabulary (~50k tokens) but this results in much smaller sequences\n",
    "\n",
    "in our case the char level token has only 65 tokens so the resulting sequence will be a one to one mapping of each character and length of sequence will scale linearly (which is bad)\n",
    "\n",
    "> TODO use one of the popular tokenizers later while implementing to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5310b22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "char2idx = { ch: i for i, ch in enumerate(chars) }\n",
    "idx2char = { i: ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda string: [char2idx[char] for char in string]\n",
    "decode = lambda tensor: \"\".join([idx2char[idx] for idx in tensor])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd10078",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4367bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n",
    "print(decode(data[:100].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750ad84",
   "metadata": {},
   "source": [
    "### Train - Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f35d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943c893",
   "metadata": {},
   "source": [
    "### hyperparameters\n",
    "\n",
    "`block_size`\n",
    "\n",
    "> we train the transformer on the above dataset as chunks, feeding in the entire dataset at once would be too computationally expensive, so we ranomly sample \"chunks\" of sequences from the dataset and train on them. The length of this sampled sequence is determined by block_size\n",
    "\n",
    "`n_vocab`\n",
    "\n",
    "> length of vocabulary, vocabulary is basically the number of unique tokens that our transformer will see and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9a0ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "seed = 1337\n",
    "batch_size = 4\n",
    "n_embedding = n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccdd0b",
   "metadata": {},
   "source": [
    "In one of these sequences, there are multiple examples packed in it. in a sequence of length 8 there are 8 unique training examples\n",
    "\n",
    "as such the `+1` is to accomodate a `y` for the last training sample, since `y` starts at an offset of `+1`\n",
    "\n",
    "### Note\n",
    "\n",
    "> The reason why multiple training samples are taken from a single sequence ranging from `1 - block_size` is not just to make it computationally efficient but to get the transformer used to seeing sequences of length in that range. `block_size` is essentially the `context_length` in transformers. During generation as well, when we keep appending generated tokens and during the next forward pass the transformer only sees the last `block_size` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abee8ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Ci\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input in: tensor([18]) the target: 47\n",
      "when input in: tensor([18, 47]) the target: 56\n",
      "when input in: tensor([18, 47, 56]) the target: 57\n",
      "when input in: tensor([18, 47, 56, 57]) the target: 58\n",
      "when input in: tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "print(decode(x.tolist()))\n",
    "print(x, y)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    \n",
    "    print(f\"when input in: {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70ee9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs, torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets, torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------------------------------------\n",
      "when input is tensor([24]), target is 43\n",
      "when input is tensor([24, 43]), target is 58\n",
      "when input is tensor([24, 43, 58]), target is 5\n",
      "when input is tensor([24, 43, 58,  5]), target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), target is 39\n",
      "when input is tensor([44]), target is 53\n",
      "when input is tensor([44, 53]), target is 56\n",
      "when input is tensor([44, 53, 56]), target is 1\n",
      "when input is tensor([44, 53, 56,  1]), target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), target is 1\n",
      "when input is tensor([52]), target is 58\n",
      "when input is tensor([52, 58]), target is 1\n",
      "when input is tensor([52, 58,  1]), target is 58\n",
      "when input is tensor([52, 58,  1, 58]), target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), target is 46\n",
      "when input is tensor([25]), target is 17\n",
      "when input is tensor([25, 17]), target is 27\n",
      "when input is tensor([25, 17, 27]), target is 10\n",
      "when input is tensor([25, 17, 27, 10]), target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_batch('train')\n",
    "print(f\"inputs, {xb.shape}\")\n",
    "print(xb)\n",
    "print(f\"targets, {yb.shape}\")\n",
    "print(yb)\n",
    "\n",
    "print('-' * 40)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context}, target is {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dfca8",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "\n",
    "For the sake of simplicity we use the simplest form of neural network, the bigram language model\n",
    "\n",
    "### Note\n",
    "\n",
    "> idx has a shape of `(B, T)`. batch, time dimensions respectively\n",
    "\n",
    "> output has a shape of `(B, T, C)`. where `C` is the embedding dimension\n",
    "\n",
    "How output becomes that shape is basically, each token idx has an associated `(65, )` dimensional vector in the embedding table, since there are 8 tokens in a sequence (block size), there will be a corresponding embedding vector for each of those tokens. this is done for all sequences in the batch (4 sequence in a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d45a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) torch.Size([])\n",
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class BigramLangugeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(n_vocab, n_embedding)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits: torch.Tensor = self.token_embedding_table(idx)\n",
    "        \n",
    "        # logits is of shape (B, T, C) however cross entropy loss expects (B, C, T)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "            \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # (B, T, C)\n",
    "            # since bigram language model, we only care about the \n",
    "            # token at previous time step\n",
    "            logits = logits[:, -1, :] # last time step -1\n",
    "            \n",
    "            # softmax to calculate probabilities along the rows (time dimension)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) generate next token for each batch element\n",
    "            \n",
    "            # append predicted token to running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T + 1) add new token to each sequence in the batch\n",
    "        return idx\n",
    "    \n",
    "m = BigramLangugeModel(n_vocab, n_embedding)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss.shape)\n",
    "\n",
    "print(logits)\n",
    "print(loss)\n",
    "    \n",
    "    \n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c722e",
   "metadata": {},
   "source": [
    "### Training Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bc390f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.509823799133301\n",
      "\n",
      "xiKi-RJ:COpVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
      "3Q&sGlvHQ?mqSq-eON\n",
      "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
      "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNYWA'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
      "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HevijuEOpmZJyF$Fwfy!PlvWPFC\n",
      "&WDdP!Ko,px\n",
      "x\n",
      "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
      "pTErIBjx;JKIgoCnLGXrJSP!Ac-rdbczR?\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "    \n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bc973",
   "metadata": {},
   "source": [
    "### Mathematical Trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6de4b301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0783, 0.4956],\n",
       "        [0.6231, 0.4224],\n",
       "        [0.2004, 0.0287],\n",
       "        [0.5851, 0.6967],\n",
       "        [0.1761, 0.2595],\n",
       "        [0.7086, 0.5809],\n",
       "        [0.0574, 0.7669],\n",
       "        [0.8778, 0.2434]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.rand(B, T, C)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041e94a",
   "metadata": {},
   "source": [
    "using simple aggregation, we can simply average all channels of tokens in the past\n",
    "\n",
    "It would become the feature vector that summarizes the particular token in the context of its previous tokens. however spacial arrangement information is lost, but for the sake of simplicity we can settle with this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92f81a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0783, 0.4956],\n",
       "         [0.3507, 0.4590],\n",
       "         [0.3006, 0.3156],\n",
       "         [0.3717, 0.4108],\n",
       "         [0.3326, 0.3806],\n",
       "         [0.3953, 0.4140],\n",
       "         [0.3470, 0.4644],\n",
       "         [0.4134, 0.4368]],\n",
       "\n",
       "        [[0.6005, 0.7079],\n",
       "         [0.5554, 0.5572],\n",
       "         [0.6657, 0.4908],\n",
       "         [0.7234, 0.6090],\n",
       "         [0.5817, 0.6344],\n",
       "         [0.6161, 0.6865],\n",
       "         [0.5946, 0.7081],\n",
       "         [0.5362, 0.6462]],\n",
       "\n",
       "        [[0.2944, 0.3677],\n",
       "         [0.3887, 0.5215],\n",
       "         [0.4333, 0.5322],\n",
       "         [0.4675, 0.4611],\n",
       "         [0.4948, 0.5105],\n",
       "         [0.4255, 0.5525],\n",
       "         [0.4748, 0.4842],\n",
       "         [0.4875, 0.5094]],\n",
       "\n",
       "        [[0.9100, 0.7684],\n",
       "         [0.8118, 0.4135],\n",
       "         [0.7959, 0.5978],\n",
       "         [0.8454, 0.6482],\n",
       "         [0.6993, 0.6530],\n",
       "         [0.5973, 0.6449],\n",
       "         [0.5726, 0.5670],\n",
       "         [0.5115, 0.5632]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a2632",
   "metadata": {},
   "source": [
    "using matrix multiplication for weighted aggregation for computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05151725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[4., 1.],\n",
      "        [6., 8.],\n",
      "        [6., 9.]])\n",
      "tensor([[4.0000, 1.0000],\n",
      "        [5.0000, 4.5000],\n",
      "        [5.3333, 6.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a /= a.sum(dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2), dtype=torch.float)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(a @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a77542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei /= torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "\n",
    "torch.allclose(xbow2, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db950c7",
   "metadata": {},
   "source": [
    "### version 3\n",
    "\n",
    "Rewriting the above logic\n",
    "\n",
    "now lets look at what these stuff actually means\n",
    "\n",
    "> `wei` - you can think of this as the interaction strength/affinity score of each token, each value in a row following up to `n-th` element would tell us how much information is flowing from `0 - n` elements to `n`\n",
    "\n",
    "> `tril` - is used in masked_fill to make sure that a particular token at time-step t only interacts with its preceeding elements, we don't want the token to look into the future tokens. This is the core difference between an encoder and a decoder\n",
    "\n",
    "> `softmax` - is a way to normalize\n",
    "\n",
    "Now for this dummy case, the \"Affinity scores\", `wei` was initialized to 0, but in practice each of these time-steps would have different affinity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12270d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before softmax\n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "after softmax\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(\"before softmax\\n\", wei)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(\"after softmax\\n\", wei)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
