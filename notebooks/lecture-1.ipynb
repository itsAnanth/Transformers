{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450dafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "A\n",
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:150])\n",
    "print(f\"Length of text: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e7fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "n_vocab = len(chars)\n",
    "\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(f\"Vocabulary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb8eb5",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "We have a custom tokenizer, its a character level tokenizer for the sake of simplicity\n",
    "\n",
    "Some popular tokenizers includes tiktoken (byte pair encoding), sentencepiece (sub word unit encoding)\n",
    "\n",
    "The above stated tokenizers have very large vocabulary (~50k tokens) but this results in much smaller sequences\n",
    "\n",
    "in our case the char level token has only 65 tokens so the resulting sequence will be a one to one mapping of each character and length of sequence will scale linearly (which is bad)\n",
    "\n",
    "> TODO use one of the popular tokenizers later while implementing to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5310b22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "char2idx = { ch: i for i, ch in enumerate(chars) }\n",
    "idx2char = { i: ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda string: [char2idx[char] for char in string]\n",
    "decode = lambda tensor: \"\".join([idx2char[idx] for idx in tensor])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd10078",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4367bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n",
    "print(decode(data[:100].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750ad84",
   "metadata": {},
   "source": [
    "### Train - Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f35d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943c893",
   "metadata": {},
   "source": [
    "### hyperparameters\n",
    "\n",
    "`block_size`\n",
    "\n",
    "> we train the transformer on the above dataset as chunks, feeding in the entire dataset at once would be too computationally expensive, so we ranomly sample \"chunks\" of sequences from the dataset and train on them. The length of this sampled sequence is determined by block_size\n",
    "\n",
    "`n_vocab`\n",
    "\n",
    "> length of vocabulary, vocabulary is basically the number of unique tokens that our transformer will see and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9a0ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccdd0b",
   "metadata": {},
   "source": [
    "In one of these sequences, there are multiple examples packed in it. in a sequence of length 8 there are 8 unique training examples\n",
    "\n",
    "as such the `+1` is to accomodate a `y` for the last training sample, since `y` starts at an offset of `+1`\n",
    "\n",
    "### Note\n",
    "\n",
    "> The reason why multiple training samples are taken from a single sequence ranging from `1 - block_size` is not just to make it computationally efficient but to get the transformer used to seeing sequences of length in that range. `block_size` is essentially the `context_length` in transformers. During generation as well, when we keep appending generated tokens and during the next forward pass the transformer only sees the last `block_size` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abee8ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Ci\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input in: tensor([18]) the target: 47\n",
      "when input in: tensor([18, 47]) the target: 56\n",
      "when input in: tensor([18, 47, 56]) the target: 57\n",
      "when input in: tensor([18, 47, 56, 57]) the target: 58\n",
      "when input in: tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "print(decode(x.tolist()))\n",
    "print(x, y)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    \n",
    "    print(f\"when input in: {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
