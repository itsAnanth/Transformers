{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "450dafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "A\n",
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:150])\n",
    "print(f\"Length of text: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8e7fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "n_vocab = len(chars)\n",
    "\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(f\"Vocabulary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb8eb5",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "We have a custom tokenizer, its a character level tokenizer for the sake of simplicity\n",
    "\n",
    "Some popular tokenizers includes tiktoken (byte pair encoding), sentencepiece (sub word unit encoding)\n",
    "\n",
    "The above stated tokenizers have very large vocabulary (~50k tokens) but this results in much smaller sequences\n",
    "\n",
    "in our case the char level token has only 65 tokens so the resulting sequence will be a one to one mapping of each character and length of sequence will scale linearly (which is bad)\n",
    "\n",
    "> TODO use one of the popular tokenizers later while implementing to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5310b22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "char2idx = { ch: i for i, ch in enumerate(chars) }\n",
    "idx2char = { i: ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda string: [char2idx[char] for char in string]\n",
    "decode = lambda tensor: \"\".join([idx2char[idx] for idx in tensor])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd10078",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4367bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n",
    "print(decode(data[:100].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750ad84",
   "metadata": {},
   "source": [
    "### Train - Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f35d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943c893",
   "metadata": {},
   "source": [
    "### hyperparameters\n",
    "\n",
    "`block_size`\n",
    "\n",
    "> we train the transformer on the above dataset as chunks, feeding in the entire dataset at once would be too computationally expensive, so we ranomly sample \"chunks\" of sequences from the dataset and train on them. The length of this sampled sequence is determined by block_size\n",
    "\n",
    "`n_vocab`\n",
    "\n",
    "> length of vocabulary, vocabulary is basically the number of unique tokens that our transformer will see and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9a0ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "seed = 1337\n",
    "batch_size = 4\n",
    "n_embedding = n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccdd0b",
   "metadata": {},
   "source": [
    "In one of these sequences, there are multiple examples packed in it. in a sequence of length 8 there are 8 unique training examples\n",
    "\n",
    "as such the `+1` is to accomodate a `y` for the last training sample, since `y` starts at an offset of `+1`\n",
    "\n",
    "### Note\n",
    "\n",
    "> The reason why multiple training samples are taken from a single sequence ranging from `1 - block_size` is not just to make it computationally efficient but to get the transformer used to seeing sequences of length in that range. `block_size` is essentially the `context_length` in transformers. During generation as well, when we keep appending generated tokens and during the next forward pass the transformer only sees the last `block_size` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "abee8ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Ci\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input in: tensor([18]) the target: 47\n",
      "when input in: tensor([18, 47]) the target: 56\n",
      "when input in: tensor([18, 47, 56]) the target: 57\n",
      "when input in: tensor([18, 47, 56, 57]) the target: 58\n",
      "when input in: tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input in: tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "print(decode(x.tolist()))\n",
    "print(x, y)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    \n",
    "    print(f\"when input in: {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "70ee9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs, torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets, torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------------------------------------\n",
      "when input is tensor([24]), target is 43\n",
      "when input is tensor([24, 43]), target is 58\n",
      "when input is tensor([24, 43, 58]), target is 5\n",
      "when input is tensor([24, 43, 58,  5]), target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), target is 39\n",
      "when input is tensor([44]), target is 53\n",
      "when input is tensor([44, 53]), target is 56\n",
      "when input is tensor([44, 53, 56]), target is 1\n",
      "when input is tensor([44, 53, 56,  1]), target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), target is 1\n",
      "when input is tensor([52]), target is 58\n",
      "when input is tensor([52, 58]), target is 1\n",
      "when input is tensor([52, 58,  1]), target is 58\n",
      "when input is tensor([52, 58,  1, 58]), target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), target is 46\n",
      "when input is tensor([25]), target is 17\n",
      "when input is tensor([25, 17]), target is 27\n",
      "when input is tensor([25, 17, 27]), target is 10\n",
      "when input is tensor([25, 17, 27, 10]), target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_batch('train')\n",
    "print(f\"inputs, {xb.shape}\")\n",
    "print(xb)\n",
    "print(f\"targets, {yb.shape}\")\n",
    "print(yb)\n",
    "\n",
    "print('-' * 40)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context}, target is {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dfca8",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "\n",
    "For the sake of simplicity we use the simplest form of neural network, the bigram language model\n",
    "\n",
    "### Note\n",
    "\n",
    "> idx has a shape of `(B, T)`. batch, time dimensions respectively\n",
    "\n",
    "> output has a shape of `(B, T, C)`. where `C` is the embedding dimension\n",
    "\n",
    "How output becomes that shape is basically, each token idx has an associated `(65, )` dimensional vector in the embedding table, since there are 8 tokens in a sequence (block size), there will be a corresponding embedding vector for each of those tokens. this is done for all sequences in the batch (4 sequence in a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9d45a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) torch.Size([])\n",
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class BigramLangugeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(n_vocab, n_embedding)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits: torch.Tensor = self.token_embedding_table(idx)\n",
    "        \n",
    "        # logits is of shape (B, T, C) however cross entropy loss expects (B, C, T)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "            \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # (B, T, C)\n",
    "            # since bigram language model, we only care about the \n",
    "            # token at previous time step\n",
    "            logits = logits[:, -1, :] # last time step -1\n",
    "            \n",
    "            # softmax to calculate probabilities along the rows (time dimension)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) generate next token for each batch element\n",
    "            \n",
    "            # append predicted token to running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T + 1) add new token to each sequence in the batch\n",
    "        return idx\n",
    "    \n",
    "m = BigramLangugeModel(n_vocab, n_embedding)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss.shape)\n",
    "\n",
    "print(logits)\n",
    "print(loss)\n",
    "    \n",
    "    \n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c722e",
   "metadata": {},
   "source": [
    "### Training Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6bc390f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.509823799133301\n",
      "\n",
      "xiKi-RJ:COpVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
      "3Q&sGlvHQ?mqSq-eON\n",
      "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
      "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNYWA'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
      "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HevijuEOpmZJyF$Fwfy!PlvWPFC\n",
      "&WDdP!Ko,px\n",
      "x\n",
      "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
      "pTErIBjx;JKIgoCnLGXrJSP!Ac-rdbczR?\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "    \n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bc973",
   "metadata": {},
   "source": [
    "### Mathematical Trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6de4b301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0783, 0.4956],\n",
       "        [0.6231, 0.4224],\n",
       "        [0.2004, 0.0287],\n",
       "        [0.5851, 0.6967],\n",
       "        [0.1761, 0.2595],\n",
       "        [0.7086, 0.5809],\n",
       "        [0.0574, 0.7669],\n",
       "        [0.8778, 0.2434]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.rand(B, T, C)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041e94a",
   "metadata": {},
   "source": [
    "using simple aggregation, we can simply average all channels of tokens in the past\n",
    "\n",
    "It would become the feature vector that summarizes the particular token in the context of its previous tokens. however spacial arrangement information is lost, but for the sake of simplicity we can settle with this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92f81a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0783, 0.4956],\n",
       "         [0.3507, 0.4590],\n",
       "         [0.3006, 0.3156],\n",
       "         [0.3717, 0.4108],\n",
       "         [0.3326, 0.3806],\n",
       "         [0.3953, 0.4140],\n",
       "         [0.3470, 0.4644],\n",
       "         [0.4134, 0.4368]],\n",
       "\n",
       "        [[0.6005, 0.7079],\n",
       "         [0.5554, 0.5572],\n",
       "         [0.6657, 0.4908],\n",
       "         [0.7234, 0.6090],\n",
       "         [0.5817, 0.6344],\n",
       "         [0.6161, 0.6865],\n",
       "         [0.5946, 0.7081],\n",
       "         [0.5362, 0.6462]],\n",
       "\n",
       "        [[0.2944, 0.3677],\n",
       "         [0.3887, 0.5215],\n",
       "         [0.4333, 0.5322],\n",
       "         [0.4675, 0.4611],\n",
       "         [0.4948, 0.5105],\n",
       "         [0.4255, 0.5525],\n",
       "         [0.4748, 0.4842],\n",
       "         [0.4875, 0.5094]],\n",
       "\n",
       "        [[0.9100, 0.7684],\n",
       "         [0.8118, 0.4135],\n",
       "         [0.7959, 0.5978],\n",
       "         [0.8454, 0.6482],\n",
       "         [0.6993, 0.6530],\n",
       "         [0.5973, 0.6449],\n",
       "         [0.5726, 0.5670],\n",
       "         [0.5115, 0.5632]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a2632",
   "metadata": {},
   "source": [
    "using matrix multiplication for weighted aggregation for computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05151725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "tensor([[8.0000, 6.0000],\n",
      "        [6.5000, 4.0000],\n",
      "        [5.6667, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a /= a.sum(dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2), dtype=torch.float)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(a @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9a77542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei /= torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "\n",
    "torch.allclose(xbow2, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db950c7",
   "metadata": {},
   "source": [
    "### version 3\n",
    "\n",
    "Rewriting the above logic\n",
    "\n",
    "now lets look at what these stuff actually means\n",
    "\n",
    "> `wei` - you can think of this as the interaction strength/affinity score of each token, each value in a row following up to `n-th` element would tell us how much information is flowing from `0 - n` elements to `n`\n",
    "\n",
    "> `tril` - is used in masked_fill to make sure that a particular token at time-step t only interacts with its preceeding elements, we don't want the token to look into the future tokens. This is the core difference between an encoder and a decoder\n",
    "\n",
    "> `softmax` - is a way to normalize\n",
    "\n",
    "Now for this dummy case, the \"Affinity scores\", `wei` was initialized to 0, but in practice each of these time-steps would have different affinity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12270d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before softmax\n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "after softmax\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "output torch.Size([4, 8, 2])\n",
      " tensor([[[0.0783, 0.4956],\n",
      "         [0.3507, 0.4590],\n",
      "         [0.3006, 0.3156],\n",
      "         [0.3717, 0.4108],\n",
      "         [0.3326, 0.3806],\n",
      "         [0.3953, 0.4140],\n",
      "         [0.3470, 0.4644],\n",
      "         [0.4134, 0.4368]],\n",
      "\n",
      "        [[0.6005, 0.7079],\n",
      "         [0.5554, 0.5572],\n",
      "         [0.6657, 0.4908],\n",
      "         [0.7234, 0.6090],\n",
      "         [0.5817, 0.6344],\n",
      "         [0.6161, 0.6865],\n",
      "         [0.5946, 0.7081],\n",
      "         [0.5362, 0.6462]],\n",
      "\n",
      "        [[0.2944, 0.3677],\n",
      "         [0.3887, 0.5215],\n",
      "         [0.4333, 0.5322],\n",
      "         [0.4675, 0.4611],\n",
      "         [0.4948, 0.5105],\n",
      "         [0.4255, 0.5525],\n",
      "         [0.4748, 0.4842],\n",
      "         [0.4875, 0.5094]],\n",
      "\n",
      "        [[0.9100, 0.7684],\n",
      "         [0.8118, 0.4135],\n",
      "         [0.7959, 0.5978],\n",
      "         [0.8454, 0.6482],\n",
      "         [0.6993, 0.6530],\n",
      "         [0.5973, 0.6449],\n",
      "         [0.5726, 0.5670],\n",
      "         [0.5115, 0.5632]]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(\"before softmax\\n\", wei)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(\"after softmax\\n\", wei)\n",
    "\n",
    "out = wei @ x\n",
    "print(f\"output {out.shape}\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ae9a6",
   "metadata": {},
   "source": [
    "### version 4\n",
    "\n",
    "self attention\n",
    "\n",
    "Now the issue with version 3 is that, we're doing a simple average of all the preceeding tokens\n",
    "this is because the affinity, `wei` is 0, the same for every token.\n",
    "\n",
    "so the resultant `wei` after normalization is uniform and basically holds equal importance. But this is not the case in real life. because different tokens will find different other tokens more or less interesting, they have to be data dependent \n",
    "\n",
    "self attention solves this issue\n",
    "\n",
    "#### How self-attention solves this\n",
    "\n",
    "Every single token at each position emits two different vectors\n",
    "\n",
    "> `Query (Q)` - tells us what the token is looking for \n",
    "\n",
    "> `Key (K)` - tells us what the particular token contains\n",
    "\n",
    "> `Value (V)` - encodes what each token wants to share with others \n",
    "\n",
    "> `out` - contextualized (context aware) representation of each token: a weighted sum (Aggregation) of other tokens' values, where the weights are given by `wei`\n",
    "\n",
    "Affinity between tokens in a sequence is achieved by performing a dot product between `Q` and `K`\n",
    "\n",
    "The dot product between the `Q` of a particular token and `K` of every other token then gives us `wei`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3ca1eef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# lets see how a single head performs self-attention\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# now interact all query with every other keys\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# we dont directly aggregate instead use another component `Value`\n",
    "# x holds the identity of the token\n",
    "# v basically tells us what information each token will communicate with us\n",
    "# out = wei @ x\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b8779",
   "metadata": {},
   "source": [
    "not how the elements are not uniform, each of them have different affinity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "154b76ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
       "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
       "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8598e-01,  2.8623e-01,\n",
       "           5.7099e-01],\n",
       "         [ 6.7643e-01, -5.4770e-01, -2.4780e-01,  3.1430e-01, -1.2799e-01,\n",
       "          -2.9521e-01, -4.2962e-01, -1.0891e-01, -4.9282e-02,  7.2679e-01,\n",
       "           7.1296e-01, -1.1639e-01,  3.2665e-01,  3.4315e-01, -7.0975e-02,\n",
       "           1.2716e+00],\n",
       "         [ 4.8227e-01, -1.0688e-01, -4.0555e-01,  1.7696e-01,  1.5811e-01,\n",
       "          -1.6967e-01,  1.6217e-02,  2.1509e-02, -2.4903e-01, -3.7725e-01,\n",
       "           2.7867e-01,  1.6295e-01, -2.8951e-01, -6.7610e-02, -1.4162e-01,\n",
       "           1.2194e+00],\n",
       "         [ 1.9708e-01,  2.8561e-01, -1.3028e-01, -2.6552e-01,  6.6781e-02,\n",
       "           1.9535e-01,  2.8073e-02, -2.4511e-01, -4.6466e-01,  6.9287e-02,\n",
       "           1.5284e-01, -2.0324e-01, -2.4789e-01, -1.6213e-01,  1.9474e-01,\n",
       "           7.6778e-01],\n",
       "         [ 2.5104e-01,  7.3457e-01,  5.9385e-01,  2.5159e-01,  2.6064e-01,\n",
       "           7.5820e-01,  5.5947e-01,  3.5387e-01, -5.9338e-01, -1.0807e+00,\n",
       "          -3.1110e-01, -2.7809e-01, -9.0541e-01,  1.3181e-01, -1.3818e-01,\n",
       "           6.3715e-01],\n",
       "         [ 3.4277e-01,  4.9605e-01,  4.7248e-01,  3.0277e-01,  1.8440e-01,\n",
       "           5.8144e-01,  3.8245e-01,  2.9521e-01, -4.8969e-01, -7.7051e-01,\n",
       "          -1.1721e-01, -2.5412e-01, -6.8921e-01,  1.9795e-01, -1.5135e-01,\n",
       "           7.6659e-01],\n",
       "         [ 1.8658e-01, -9.6351e-02, -1.4300e-01,  3.0587e-01,  8.3441e-02,\n",
       "          -6.8646e-03, -2.0472e-01, -1.5350e-01, -7.6250e-02,  3.2689e-01,\n",
       "           3.0896e-01,  7.6626e-02,  9.9243e-02,  1.6560e-01,  1.9745e-01,\n",
       "           7.6248e-01],\n",
       "         [ 1.3013e-01, -3.2832e-02, -4.9645e-01,  2.8652e-01,  2.7042e-01,\n",
       "          -2.6357e-01, -7.3756e-02,  3.7857e-01,  7.4580e-02,  3.3827e-02,\n",
       "           1.4695e-02,  3.1937e-01,  2.9926e-01, -1.6530e-01, -3.8630e-02,\n",
       "           3.3748e-01]],\n",
       "\n",
       "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6267e-03,\n",
       "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
       "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
       "          -4.4439e-01],\n",
       "         [-3.8338e-01, -1.9659e-01,  8.8455e-02,  1.8560e-01, -8.7010e-02,\n",
       "           1.3239e-01,  3.0841e-01, -2.4350e-01, -1.9396e-01, -1.7634e-02,\n",
       "           4.8439e-01,  5.4210e-01, -2.0407e-02, -4.2467e-01, -2.3463e-01,\n",
       "          -4.6465e-01],\n",
       "         [-1.1100e+00,  3.2334e-01,  4.7054e-01, -6.3595e-02,  2.5443e-01,\n",
       "           1.5352e-01,  2.5186e-01,  2.6286e-01,  2.7916e-01, -3.1662e-03,\n",
       "          -3.2881e-02,  4.8191e-01,  7.4431e-01, -1.9921e-01,  2.7134e-01,\n",
       "          -8.5871e-02],\n",
       "         [-9.7190e-01,  4.6124e-01,  4.2349e-01, -1.7230e-02,  1.5847e-01,\n",
       "           4.1175e-01,  4.0764e-01,  2.4982e-01, -5.0322e-02,  4.1514e-03,\n",
       "          -3.9853e-01,  4.3551e-01,  7.0285e-01, -4.3081e-01,  2.6684e-02,\n",
       "          -2.0169e-01],\n",
       "         [ 3.3586e-01, -8.5915e-02,  9.3660e-01,  7.7311e-01,  1.8037e-01,\n",
       "           8.2853e-01, -6.9183e-02,  2.8814e-01,  1.1734e-01,  6.8448e-01,\n",
       "          -5.8500e-02,  1.2726e-01,  2.9780e-01,  1.9324e-01,  1.5655e-01,\n",
       "          -9.3005e-03],\n",
       "         [ 1.6984e-01,  3.0993e-02,  8.1557e-01,  6.1679e-01,  1.0429e-01,\n",
       "           7.4573e-01,  2.3072e-02,  3.0572e-01,  5.8163e-02,  5.7122e-01,\n",
       "          -4.5275e-02,  1.5051e-01,  3.2901e-01,  5.6984e-02,  1.0311e-01,\n",
       "          -9.9174e-02],\n",
       "         [ 4.6496e-02,  1.5765e-01,  3.9760e-01,  1.7619e-01, -2.1168e-01,\n",
       "           2.3365e-01, -6.2083e-02,  2.1726e-01, -7.8725e-03,  4.5389e-01,\n",
       "           3.4349e-01, -5.5631e-02,  3.3726e-01, -3.7591e-01, -1.0140e-02,\n",
       "          -4.5806e-01],\n",
       "         [-5.3896e-01,  7.5555e-01,  3.3034e-01, -1.5849e-01, -2.6740e-01,\n",
       "           4.3495e-01,  3.7772e-01,  5.5794e-01, -1.8369e-01,  1.5938e-01,\n",
       "          -2.1042e-01,  5.5790e-02,  6.3184e-01, -6.4884e-01, -9.6084e-02,\n",
       "          -5.0751e-01]],\n",
       "\n",
       "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
       "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
       "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
       "          -5.5987e-01],\n",
       "         [-4.6375e-01,  6.3807e-01, -1.5842e-01, -1.3309e-01, -5.9402e-01,\n",
       "          -5.0374e-01,  2.3289e-01, -3.2126e-01,  4.5781e-01, -1.8590e-01,\n",
       "           1.9215e-01,  3.7566e-01, -3.5905e-01, -7.7262e-01,  3.5036e-01,\n",
       "           6.9694e-02],\n",
       "         [-6.4044e-01,  1.3831e-01, -6.1007e-02, -1.1112e-01, -4.5228e-01,\n",
       "          -6.2271e-01, -1.7030e-01, -2.4949e-01,  5.0670e-01, -9.6444e-02,\n",
       "           4.8315e-01,  9.4986e-02, -2.9810e-01, -3.6538e-01,  3.9458e-01,\n",
       "           4.1512e-01],\n",
       "         [-6.7193e-01,  1.2516e-01,  7.3386e-02, -1.3198e-01, -1.7880e-01,\n",
       "          -5.6740e-01, -6.8226e-01,  5.0844e-02,  3.3051e-01,  7.8242e-02,\n",
       "           6.8022e-02, -2.4041e-01, -6.6864e-02, -1.8411e-01, -5.3514e-02,\n",
       "           4.5113e-01],\n",
       "         [-1.4270e-02,  1.0195e+00, -3.4792e-01, -1.6421e-01, -5.5846e-01,\n",
       "          -3.2457e-01,  9.9404e-01, -5.6891e-01,  4.0097e-01, -1.8123e-01,\n",
       "           1.1856e-01,  9.8704e-01, -6.4057e-01, -1.0320e+00,  7.3320e-01,\n",
       "          -4.3167e-01],\n",
       "         [-6.3858e-01, -7.6533e-02, -3.6510e-01,  1.7782e-01, -6.5426e-02,\n",
       "          -3.5158e-01,  7.9591e-02,  1.7384e-01,  3.6676e-01, -4.2302e-02,\n",
       "           2.4923e-01,  4.8239e-01, -2.1295e-01, -2.9492e-01,  3.4749e-01,\n",
       "          -1.7111e-01],\n",
       "         [-2.2366e-01, -5.5317e-02, -1.8296e-01,  2.4258e-01,  2.5357e-01,\n",
       "          -1.6154e-01, -2.3908e-01,  3.3243e-01,  1.0304e-01,  2.6067e-01,\n",
       "          -5.0670e-02,  3.6947e-01, -4.9856e-02,  1.1197e-01,  1.1752e-01,\n",
       "          -2.5078e-01],\n",
       "         [-2.4821e-01,  1.4845e-01, -3.5033e-01,  1.7102e-01,  1.6613e-01,\n",
       "          -2.0643e-01,  8.6633e-02,  8.8414e-02,  2.1188e-01,  2.5805e-01,\n",
       "           5.5145e-02,  4.2668e-01, -2.0443e-01, -1.7372e-01,  3.8899e-01,\n",
       "           5.1725e-02]],\n",
       "\n",
       "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
       "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
       "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
       "           1.5391e+00],\n",
       "         [ 1.9305e-01, -2.1031e-01, -3.4658e-01,  2.0567e-01, -1.7798e-01,\n",
       "          -7.4604e-01, -6.4427e-01, -6.9183e-01, -2.0558e-01,  7.0413e-01,\n",
       "           2.3632e-01,  9.8796e-04, -1.7015e-01,  1.1203e-01, -7.1064e-01,\n",
       "           1.2431e+00],\n",
       "         [ 2.9114e-01, -4.8343e-01, -5.9254e-01,  4.6477e-01, -2.1832e-01,\n",
       "          -6.4460e-01, -1.1627e+00, -7.0993e-01, -1.9703e-01,  2.9262e-01,\n",
       "           2.3669e-01, -3.1050e-02, -1.5471e-01,  7.7153e-02, -8.1137e-01,\n",
       "           9.3578e-01],\n",
       "         [ 1.7549e-01, -3.4260e-02, -2.0523e-01,  2.7644e-02, -2.1312e-01,\n",
       "          -5.6022e-01, -3.5273e-01, -6.2722e-01, -3.0037e-01,  4.6061e-01,\n",
       "           1.5004e-01,  1.9040e-02, -1.4646e-01,  1.7220e-01, -6.2559e-01,\n",
       "           1.0722e+00],\n",
       "         [ 1.7354e-01, -1.7962e-01, -2.7874e-01, -1.0590e-01, -1.2952e-01,\n",
       "          -3.5086e-01, -5.5830e-01, -3.8638e-01, -2.9719e-01,  3.3368e-02,\n",
       "           1.7392e-01,  5.5898e-02, -7.2007e-02,  1.3182e-02, -6.6710e-01,\n",
       "           5.4229e-01],\n",
       "         [ 2.4678e-01, -4.7274e-01, -5.2827e-01,  3.1212e-01, -1.7528e-01,\n",
       "          -4.8636e-01, -1.1223e+00, -5.4196e-01, -2.0142e-01,  4.0103e-02,\n",
       "           2.2231e-01, -2.9381e-02, -9.4353e-02,  2.6374e-02, -7.8726e-01,\n",
       "           6.2836e-01],\n",
       "         [-3.9784e-01,  2.5915e-01,  5.0358e-01, -4.6864e-01, -2.2024e-02,\n",
       "          -3.2242e-01, -1.2578e-01,  1.0634e-01,  1.3618e-01,  1.7780e-01,\n",
       "           1.0391e-01, -6.2540e-01,  3.8904e-01,  3.3690e-01, -5.5140e-01,\n",
       "           5.2246e-01],\n",
       "         [-3.5927e-01,  3.3935e-02, -2.9863e-02, -1.5019e-01, -6.0354e-03,\n",
       "          -6.5733e-02, -3.9659e-01, -6.0435e-02, -5.7551e-01, -2.9157e-01,\n",
       "           1.4899e-01, -7.5002e-02,  7.3228e-02, -4.7413e-02, -6.4394e-01,\n",
       "           2.8560e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54273eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb7f33",
   "metadata": {},
   "source": [
    "In a **T x T attention score matrix** for a single batch, where **T** is the sequence length or block size, the matrix represents the attention scores computed during the self-attention mechanism (e.g., in a Transformer model). Here's what each component represents:\n",
    "\n",
    "- **Rows**: Each row corresponds to a **query token** in the sequence. For a sequence of length **T**, there are **T** rows, where the \\(i\\)-th row represents the attention scores for the \\(i\\)-th token's query vector attending to all tokens (including itself).\n",
    "\n",
    "- **Columns**: Each column corresponds to a **key token** in the sequence. The \\(j\\)-th column represents the contribution of the \\(j\\)-th token's key vector to the attention scores for all queries.\n",
    "\n",
    "- **Elements**: Each element \\(A_{i,j}\\) in the matrix represents the **attention score** between the \\(i\\)-th query token and the \\(j\\)-th key token. This score indicates how much the \\(i\\)-th token attends to the \\(j\\)-th token when computing its output representation. Typically, these scores are computed as:\n",
    "  \\[\n",
    "  A_{i,j} = \\text{score}(Q_i, K_j) = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\n",
    "  \\]\n",
    "  (for scaled dot-product attention, before softmax), where \\(Q_i\\) is the query vector for the \\(i\\)-th token, \\(K_j\\) is the key vector for the \\(j\\)-th token, and \\(d_k\\) is the dimension of the key vectors. After softmax, \\(A_{i,j}\\) represents the normalized attention weight.\n",
    "\n",
    "### Summary:\n",
    "- **Row \\(i\\)**: Attention scores for the \\(i\\)-th token's query attending to all tokens.\n",
    "- **Column \\(j\\)**: Contribution of the \\(j\\)-th token's key to all queries.\n",
    "- **Element \\(A_{i,j}\\)**: Attention score/weight for how much the \\(i\\)-th token attends to the \\(j\\)-th token.\n",
    "\n",
    "If you need further details (e.g., about softmax or masking), let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
